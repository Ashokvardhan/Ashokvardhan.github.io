# Works along the former theme:

- *Learning in Gated Neural Networks*: Inspired by the tremendous successes of gated neural networks like LSTMs, GRUs, and Attention networks in natural language processing, we designed and proved first consistent and efficient algorithms of any kind for an architecture called Mixture-of-Experts, which is at the heart of these gated neural networks: [http://proceedings.mlr.press/v97/makkuva19a.html Efficient Algorithms for Mixture-of-Experts], [https://arxiv.org/abs/1906.02777 Learning in Gated Neural Networks].\n
- *Optimal transport*: In recent years, we have also seen many interesting developments in the intersection of optimal transport and machine learning. An important problem in this area is to learn an optimal transport map (under a suitable metric) between any two given probability distribution just given their samples. For the Wasserstein-2 metric, our work here on [https://arxiv.org/abs/1908.10962 Optimal transport mapping via Input-Convex-Neural-Networks] provides a clean mathematical framework and an algorithm to learn optimal transport maps which is robust to initializations and can also learn discontinuous transport maps which are quite common in practice (such as a map transporting a Gaussian to a MNIST). \n

*Deep Code*: Along the latter theme, we are currently working on building new state-of-the-art communication codes parametrized by neural networks. The main motive behind this project is two-fold: 

- Designing new state-of-the-art codes is largely human ingenuity driven and hence the progress is sporadic. Hence our goal is to innovate this hard process with the help of neural networks. \n
- While the landmark coding schemes such as Convolutional codes, Turbo codes, LDPC codes, Polar codes, etc. are already very good under the AWGN setting, they are not fully robust if the channel changes. Hence we would like to learn new codes in a data-driven manner that is inherently robust to these changes and is as good as these codes on the AWGN. 


== Background
###